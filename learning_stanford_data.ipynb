{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. original_rt_snippets.txt contains 10,605 processed snippets from the original pool of Rotten Tomatoes HTML files. Please note that some snippet may contain multiple sentences.\n",
    "\n",
    "2. dictionary.txt contains all phrases and their IDs, separated by a vertical line |\n",
    "\n",
    "3. sentiment_labels.txt contains all phrase ids and the corresponding sentiment labels, separated by a vertical line.\n",
    "Note that you can recover the 5 classes by mapping the positivity probability using the following cut-offs:\n",
    "[0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\n",
    "for very negative, negative, neutral, positive, very positive, respectively.\n",
    "Please note that phrase ids and sentence ids are not the same.\n",
    "\n",
    "4. SOStr.txt and STree.txt encode the structure of the parse trees. \n",
    "STree encodes the trees in a parent pointer format. Each line corresponds to each sentence in the datasetSentences.txt file. The Matlab code of this paper will show you how to read this format if you are not familiar with it.\n",
    "\n",
    "5. datasetSentences.txt contains the sentence index, followed by the sentence string separated by a tab. These are the sentences of the train/dev/test sets.\n",
    "\n",
    "6. datasetSplit.txt contains the sentence index (corresponding to the index in datasetSentences.txt file) followed by the set label separated by a comma:\n",
    "\t1 = train\n",
    "\t2 = test\n",
    "\t3 = dev\n",
    "\n",
    "Please note that the datasetSentences.txt file has more sentences/lines than the original_rt_snippet.txt. \n",
    "Each row in the latter represents a snippet as shown on RT, whereas the former is each sub sentence as determined by the Stanford parser.\n",
    "\n",
    "For comparing research and training models, please use the provided train/dev/test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sneakpeak into the files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Hook spark up\n",
    "import os\n",
    "import sys\n",
    "spark_home = '/home/dharashah/spark'#os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, '/home/dharashah/spark/python/lib/py4j-0.10.4-src.zip')) \n",
    "print spark_home\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.11:1.5.0'\n",
    "#from pyspark.sql import SQLContext\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_files_from_dir(dirpath):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    onlyfiles = [f for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "    return onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/home/dharashah/Documents/Spring_18/DeepLearning/project/classify_other_data/stanfordSentimentTreebank/'\n",
    "files = get_all_files_from_dir(filepath)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# look at hte given files, join the sentences and the labels \n",
    "\n",
    "snipptfile = 'original_rt_snippets.txt'\n",
    "labelsfile = 'sentiment_labels.txt'\n",
    "splitfile = 'datasetSplit.txt'\n",
    "dictionaryfile = 'dictionary.txt'\n",
    "datasetsentencesfile = 'datasetSentences.txt'\n",
    "files = [snipptfile,\n",
    "labelsfile,\n",
    "splitfile,\n",
    "dictionaryfile,\n",
    "datasetsentencesfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt\n",
      "---------------------------------------\n",
      "Stanford Sentiment Treebank V1.0\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "STree.txt\n",
      "---------------------------------------\n",
      "70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0\n",
      "\n",
      "71|70|69|69|67|67|66|64|63|62|62|61|61|58|57|57|56|53|53|52|52|49|49|50|48|45|44|43|43|42|42|41|39|38|38|40|60|39|40|41|47|46|44|45|46|47|48|51|50|51|55|54|54|55|56|59|58|59|60|73|65|63|64|65|66|68|68|72|70|71|72|73|0\n",
      "\n",
      "-----------------------------------\n",
      "datasetSplit.txt\n",
      "---------------------------------------\n",
      "sentence_index,splitset_label\n",
      "\n",
      "1,1\n",
      "\n",
      "-----------------------------------\n",
      "sentiment_labels.txt\n",
      "---------------------------------------\n",
      "phrase ids|sentiment values\n",
      "\n",
      "0|0.5\n",
      "\n",
      "-----------------------------------\n",
      "original_rt_snippets.txt\n",
      "---------------------------------------\n",
      "The Rock is destined to be the 21st Century's new ``Conan'' and that he's going to make a splash even greater than Arnold Schwarzenegger, Jean-Claud Van Damme or Steven Segal.\n",
      "\n",
      "The gorgeously elaborate continuation of ``The Lord of the Rings'' trilogy is so huge that a column of words cannot adequately describe co-writer/director Peter Jackson's expanded vision of J.R.R. Tolkien's Middle-earth.\n",
      "\n",
      "-----------------------------------\n",
      "SOStr.txt\n",
      "---------------------------------------\n",
      "The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\n",
      "\n",
      "The|gorgeously|elaborate|continuation|of|``|The|Lord|of|the|Rings|''|trilogy|is|so|huge|that|a|column|of|words|can|not|adequately|describe|co-writer\\/director|Peter|Jackson|'s|expanded|vision|of|J.R.R.|Tolkien|'s|Middle-earth|.\n",
      "\n",
      "-----------------------------------\n",
      "datasetSentences.txt\n",
      "---------------------------------------\n",
      "sentence_index\tsentence\n",
      "\n",
      "1\tThe Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "\n",
      "-----------------------------------\n",
      "dictionary.txt\n",
      "---------------------------------------\n",
      "!|0\n",
      "\n",
      "! '|22935\n",
      "\n",
      "-----------------------------------\n",
      "read_stanford_data.py\n",
      "---------------------------------------\n",
      "\"\"\"\n",
      "\n",
      "Put all the Stanford Sentiment Treebank phrase data into test, training, and dev CSVs.\n",
      "\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "def head_n(fname, n=2):\n",
    "    count = 1\n",
    "    print '---------------------------------------'\n",
    "    f = open(fname)\n",
    "    for line in f:\n",
    "        print line\n",
    "        count += 1\n",
    "        if count > n:\n",
    "            print '-----------------------------------'\n",
    "            f.close()\n",
    "            return\n",
    "    \n",
    "for filename in files:\n",
    "    print filename\n",
    "    head_n(filepath+filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "!|0\n",
      "\n",
      "! '|22935\n",
      "\n",
      "! ''|18235\n",
      "\n",
      "! Alas|179257\n",
      "\n",
      "! Brilliant|22936\n",
      "\n",
      "! Brilliant !|40532\n",
      "\n",
      "! Brilliant ! '|22937\n",
      "\n",
      "! C'mon|60624\n",
      "\n",
      "! Gollum 's ` performance ' is incredible|13402\n",
      "\n",
      "! Oh , look at that clever angle ! Wow , a jump cut !|179258\n",
      "\n",
      "! Romething|140882\n",
      "\n",
      "! Run|179259\n",
      "\n",
      "! The Movie|60625\n",
      "\n",
      "! The camera twirls ! Oh , look at that clever angle ! Wow , a jump cut !|179260\n",
      "\n",
      "! True Hollywood Story|140883\n",
      "\n",
      "! Wow|179261\n",
      "\n",
      "! Zoom !|179262\n",
      "\n",
      "!?|220445\n",
      "\n",
      "!? '|220446\n",
      "\n",
      "#|60626\n",
      "\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# looks like the sentiment lables can go with the datasetSentences. varify it to begin with \n",
    "\n",
    "snipptfile = 'original_rt_snippets.txt'\n",
    "labelsfile = 'sentiment_labels.txt'\n",
    "splitfile = 'datasetSplit.txt'\n",
    "dictionaryfile = 'dictionary.txt'\n",
    "datasetsentencesfile = 'datasetSentences.txt'\n",
    "head_n(filepath+dictionaryfile,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_id_sent_for_joining(sent_list):\n",
    "    try:\n",
    "        i = int(sent_list[0])\n",
    "        return (i,sent_list[1])\n",
    "    except:\n",
    "        return (0,'title')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# see if you can use the datasplit for the performance \n",
    "id_sent = sc.textFile(filepath+datasetsentencesfile).map(lambda x:x.split('\\t')).map(lambda x:filter_id_sent_for_joining(x))\n",
    "id_label = sc.textFile(filepath+labelsfile).map(lambda x:x.split(\"|\")).map(lambda x:filter_id_sent_for_joining(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#id_sent.take(10)\n",
    "#id_label.take(10)\n",
    "print id_sent.count()\n",
    "print id_label.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Division into cross validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the script was provided by authors of the \"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\"\n",
    "### after running the script saved as 'read_stanford_data.py' with a few changes for target directory and saving address, \n",
    "### the datasets are as in folder 'cross_validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# peak into the cross_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dharashah/Documents/Spring_18/DeepLearning/project/classify_other_data/stanfordSentimentTreebank/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = get_all_files_from_dir(filepath+'cross_validation_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stanford-sentiment-treebank.train.csv',\n",
       " 'stanford-sentiment-treebank.test.csv',\n",
       " 'stanford-sentiment-treebank.dev.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "train_df = pd.read_csv(filepath+'cross_validation_data/'+files[0])\n",
    "test_df = pd.read_csv(filepath+'cross_validation_data/'+files[1])\n",
    "val_df = pd.read_csv(filepath+'cross_validation_data/'+files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: very positive, very negative\n",
      "14713 10952\n",
      "validation: very positive, very negative\n",
      "158 132\n",
      "testing: very positive, very negative\n",
      "385 270\n"
     ]
    }
   ],
   "source": [
    "#filter the dataframe for very positive and very negative \n",
    "#df.loc[df['column_name'] == some_value]\n",
    "def _get_very_positive_very_negative(df):\n",
    "    vp_df = df.loc[df['fine'] == 'very positive']\n",
    "    vn_df = df.loc[df['fine'] == 'very negative']\n",
    "    return vp_df,vn_df\n",
    "    \n",
    "\n",
    "vp_train_df,vn_train_df = _get_very_positive_very_negative(train_df)\n",
    "print 'training: very positive, very negative'\n",
    "print len(vp_train_df),len(vn_train_df)\n",
    "vp_val_df,vn_val_df = _get_very_positive_very_negative(val_df)\n",
    "print 'validation: very positive, very negative'\n",
    "print len(vp_val_df),len(vn_val_df)\n",
    "vp_test_df,vn_test_df = _get_very_positive_very_negative(test_df)\n",
    "print 'testing: very positive, very negative'\n",
    "print len(vp_test_df),len(vn_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatinate_frames(df1,df2):\n",
    "    c =  pd.concat([df1,df2]) \n",
    "    assert len(c) == len(df1)+len(df2)\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vpn_train_df =concatinate_frames (vp_train_df,vn_train_df)\n",
    "vpn_val_df = concatinate_frames(vp_val_df,vn_val_df)\n",
    "vpn_test_df = concatinate_frames(vp_test_df,vn_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dharashah/Documents/Spring_18/DeepLearning/project/classify_other_data/stanfordSentimentTreebank/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stanford-sentiment-treebank.train.csv',\n",
       " 'stanford-sentiment-treebank.test.csv',\n",
       " 'stanford-sentiment-treebank.dev.csv']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right now, just hook the RNN up, we will see what happens later.\n",
    "#Write the data\n",
    "def write_as_csv(targetpath,df,sep=','):\n",
    "    df.to_csv(targetpath,sep)\n",
    "    print 'done!'\n",
    "print filepath\n",
    "cross_val_files =  get_all_files_from_dir(filepath+'cross_validation_data')\n",
    "cross_val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = {'stanford-sentiment-treebank.train.vpn.csv':vpn_train_df,\n",
    "    'stanford-sentiment-treebank.train.vp.csv':vp_train_df,\n",
    "        'stanford-sentiment-treebank.train.vn.csv':vn_train_df,\n",
    "         'stanford-sentiment-treebank.dev.vpn.csv':vpn_val_df,\n",
    "         'stanford-sentiment-treebank.dev.vp.csv':vp_val_df,\n",
    "         'stanford-sentiment-treebank.dev.vn.csv':vn_val_df,\n",
    "         'stanford-sentiment-treebank.test.vpn.csv':vpn_test_df,\n",
    "         'stanford-sentiment-treebank.test.vp.csv':vp_test_df,\n",
    "         'stanford-sentiment-treebank.test.vn.csv':vn_test_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "for n in names.keys():\n",
    "    write_as_csv(filepath+'cross_validation_data/'+n,names[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
