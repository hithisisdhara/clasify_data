{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I had to install python 3.5 environment in my anaconda2 in order to install and run ekphasis\n",
    "# Python 3 environment is installed as py35 \n",
    "# I installed pip install -r requirements.txt\n",
    "# I run pip3 install local_installation.sh \n",
    "#-------------------------------------------------------------------------------\n",
    "# in order to run this program on my school computer, first open the commandline\n",
    "# type 'sorce activate py35'\n",
    "# once you see py35[place you are at]$, you know you are in py35 env\n",
    "# now you type 'jupyter notebook' to open jupyter in python 3.5 shell \n",
    "# open this file, and run the cells. \n",
    "# type 'sorce deactivate py35' to get oput of this shell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "<allcaps> cant wait </allcaps> for the new season of <hashtag> twin peaks </hashtag> ＼(^o^)／ ! <repeated> <hashtag> david lynch </hashtag> <hashtag> tv series </hashtag> <happy>\n",
      "i saw the new <hashtag> john doe </hashtag> movie and it sucks <elongated> ! <repeated> <allcaps> waisted </allcaps> <money> . <repeated> <hashtag> bad movies </hashtag> <annoyed>\n",
      "<user> : can not wait for the <date> <hashtag> sentiment </hashtag> talks ! <allcaps> yay <elongated> </allcaps> ! <repeated> <laugh> <url>\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
    "    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
    "    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    print(\" \".join(text_processor.pre_process_doc(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so <elongated>\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('sooooooo')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<money> to me\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('$300 to me')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<date>\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('3/4/15')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<date>\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('March 3')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<user>\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('@hithisisdhara')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<url>\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('https://mail.google.com/mail/u/0/#inbox')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hashtag> gsu </hashtag>\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('#GSU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loll\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('loll')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<happy>\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc(':)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top <number>\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('top 20')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top20\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('top20')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "＼(^o^)／\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(text_processor.pre_process_doc('＼(^o^)／')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "def clean_text(text):\n",
    "    text = text.rstrip()\n",
    "\n",
    "    if '\"\"' in text:\n",
    "        if text[0] == text[-1] == '\"':\n",
    "            text = text[1:-1]\n",
    "        text = text.replace('\\\\\"\"', '\"')\n",
    "        text = text.replace('\"\"', '\"')\n",
    "\n",
    "    text = text.replace('\\\\\"\"', '\"')\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/dharashah/Documents/Spring_18/DeepLearning/project/datastories/datastories-semeval2017-task4/dataset/Subtask_BD/4B-English/SemEval2017-task4-dev.subtask-BD.english.INPUT.txt')\n",
    "cleaned_data = [] \n",
    "for line in f:\n",
    "    tokens = line.rstrip().split('\\t')\n",
    "    tweet = clean_text(tokens[-1])\n",
    "    #print(tweet)\n",
    "    processed_tweet = \" \".join(text_processor.pre_process_doc(tweet))\n",
    "    cleaned_data.append(tokens[:-1]+[processed_tweet])\n",
    "    #break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10552"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/dharashah/Documents/Spring_18/DeepLearning/project'\n",
    "f = open(filepath+'/'+'cleaned_data_tab_separatex.txt','w')\n",
    "for c in cleaned_data:\n",
    "    line =  '\\t'.join(c)\n",
    "    f.write(line+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635291585110929408\tcaitlyn jenner\tnegative\tthere ' s a <number> / <number> % chance that caitlyn jenner may be charged with manslaughter i should think so as well after what she did to poor bruce\n",
      "\n",
      "628569856246308864\tdavid price\tpositive\t\" no matter what drake tells you , monday in <hashtag> toronto </hashtag> belonged to david price . <hashtag> jays </hashtag> - <url> <url>\n",
      "\n",
      "640162813625942016\tfoo fighters\tpositive\tseeing foo fighters tomorrow argh <elongated> i am excited and terrified at the same time\n",
      "\n",
      "665090003257683968\tira\tnegative\t\" just me that thinks it ' s fucking daft arresting oaps for killings on bloody sunday , too long ago , fuck the <allcaps> ira </allcaps> \"\n",
      "\n",
      "681696455736733696\tjustin bieber\tnegative\tme : \" justin bieber is gonna be in that 2 nd zoolander movie but only for a quick scene cause he dies \" dad : \" good \" <user>\n",
      "\n",
      "675890017487724544\tlady gaga\tpositive\twe want gaga in 1 st <hashtag> video mtv 2015 </hashtag> fifth harmony <hashtag> mtv stars </hashtag> lady gaga\n",
      "\n",
      "639540571422502913\tniall\tnegative\tremember when that woman was \" giving birth \" during that prank and niall just fucking sat there\n",
      "\n",
      "637714326103523328\tps4\tpositive\t<user> and i am serious when i say this : this may just be the best exclusive on ps4 ! well done mr . yoshida ! well done !\n",
      "\n",
      "624430548811911168\tsharknado\tpositive\tfrom the makers of the sharknado franchise comes a new movie on saturday called lavalantula . i must see this .\n",
      "\n",
      "632217761700315136\tu2\tpositive\t\" this friday evening is brought to you by the strains of eric clapton . well played , ipod . sucking up to me after playing u2 yesterday \"\n",
      "\n",
      "10552\n"
     ]
    }
   ],
   "source": [
    "# sainitychk\n",
    "counter = 0 \n",
    "f = open(filepath+'/'+'cleaned_data_tab_separatex.txt')\n",
    "for line in f:\n",
    "    counter += 1 \n",
    "    if counter % 1000 == 0:\n",
    "        print(line)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
